{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Subham\\Anaconda2\\envs\\keras_env\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import quandl #http://127.0.0.1:8888/?token=09794f0b843849dac8d164691e7fb4f9c2818455909b9f6e\n",
    "#token\n",
    "import numpy as np\n",
    "from sklearn import preprocessing,cross_validation, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#here we are using slearn for scaling the data \n",
    "# scaling is usually done on features to keep it between \n",
    "#-1 to 1 it helps in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= quandl.get('WIKI/GOOGL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open    High     Low    Close      Volume  Ex-Dividend  \\\nDate                                                                   \n2004-08-19  100.01  104.06   95.96  100.335  44659000.0          0.0   \n2004-08-20  101.01  109.08  100.50  108.310  22834300.0          0.0   \n2004-08-23  110.76  113.48  109.05  109.400  18256100.0          0.0   \n2004-08-24  111.24  111.60  103.57  104.870  15247300.0          0.0   \n2004-08-25  104.76  108.00  103.88  106.000   9188600.0          0.0   \n\n            Split Ratio  Adj. Open  Adj. High   Adj. Low  Adj. Close  \\\nDate                                                                   \n2004-08-19          1.0  50.159839  52.191109  48.128568   50.322842   \n2004-08-20          1.0  50.661387  54.708881  50.405597   54.322689   \n2004-08-23          1.0  55.551482  56.915693  54.693835   54.869377   \n2004-08-24          1.0  55.792225  55.972783  51.945350   52.597363   \n2004-08-25          1.0  52.542193  54.167209  52.100830   53.164113   \n\n            Adj. Volume  \nDate                     \n2004-08-19   44659000.0  \n2004-08-20   22834300.0  \n2004-08-23   18256100.0  \n2004-08-24   15247300.0  \n2004-08-25    9188600.0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "# to check the features\n",
    "# try to think about relationships between algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Adj. Close  PCT_change    HL_PCT  Adj. Volume\nDate                                                     \n2004-08-19   50.322842    0.324968  3.712563   44659000.0\n2004-08-20   54.322689    7.227007  0.710922   22834300.0\n2004-08-23   54.869377   -1.227880  3.729433   18256100.0\n2004-08-24   52.597363   -5.726357  6.417469   15247300.0\n2004-08-25   53.164113    1.183658  1.886792    9188600.0\n"
     ]
    }
   ],
   "source": [
    "# garbbing features\n",
    "df = df [['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']]\n",
    "df['HL_PCT']= (df['Adj. High']-df['Adj. Close'])/ df['Adj. Close']*100.0\n",
    "df['PCT_change']= (df['Adj. Close']-df['Adj. Open'])/ df['Adj. Open']*100.0\n",
    "\n",
    "df=df[['Adj. Close','PCT_change','HL_PCT','Adj. Volume']]\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Adj. Close  PCT_change    HL_PCT  Adj. Volume      label\nDate                                                                \n2004-08-19   50.322842    0.324968  3.712563   44659000.0  68.752232\n2004-08-20   54.322689    7.227007  0.710922   22834300.0  69.639972\n2004-08-23   54.869377   -1.227880  3.729433   18256100.0  69.078238\n2004-08-24   52.597363   -5.726357  6.417469   15247300.0  67.839414\n2004-08-25   53.164113    1.183658  1.886792    9188600.0  68.912727\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "forecast_col = 'Adj. Close'\n",
    "#just in case there is missing data\n",
    "df.fillna(-99999, inplace=True)\n",
    "#in machine learning we cannot work with Nan so we give\n",
    "#it a outliner value or remove the data \n",
    "# and since we  alredy dont have enough data in the\n",
    "# world we cannot afford to lose more data\n",
    "\n",
    "forecast_out=int(math.ceil(0.01*len(df)))# no of days out\n",
    "# tring to prdict out 10% of the size of your data frame\n",
    "# math.ceil returns float and we dont want that\n",
    "\n",
    "############# we have features  noe we need lables####\n",
    "\n",
    "df['label']= df[forecast_col].shift(-forecast_out)\n",
    "#so what we did here was that in order to create the lable \n",
    "#lables we need to understand what lables are \n",
    "# lables are information for the algorithm which says \n",
    "#for this perticular input value the predicted value \n",
    "# is that label\n",
    "df.dropna(inplace=True)\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3178 3178\n"
     ]
    }
   ],
   "source": [
    "# generally feaatures nad lables are dinoted by x and y\n",
    "# X = np.array(df.drop(['label'],1))\n",
    "# y = np.array(df['label'])\n",
    "# scale x\n",
    "# X = preprocessing.scale(X)\n",
    "# X - features\n",
    "# y- labels\n",
    "# why is scaling time consuming ?\n",
    "# now that we have scaled our features and passed it to a classifier and we have now trainedour classifier ,\n",
    "#  now that new dat enters the system while testing \n",
    "# we need to make sure we scale the new data too before sending it into the clssifier but the \n",
    "# problem here is that we wil have to scale the new datt \n",
    "# we all the training dat to get the correct scaling\n",
    "\n",
    "# X = X[:-forecast_out+1]# we are making sure that we use X's for which we have values in yy\n",
    "# as you might remember we had shifted the Adj. Close by forecats out\n",
    "# df.dropna(inplace=True)\n",
    "# y= np.array(df['label'])\n",
    "# print (len(X),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972837852519\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size=0.2)\n",
    "# clf = LinearRegression(n_jobs=-1)#no of treads that linear regresion supports for traing\n",
    "# clf.fit(X_train,y_train)\n",
    "# accuracy=clf.score(X_test,y_test)\n",
    "# print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3145 3145\n"
     ]
    }
   ],
   "source": [
    "############regresion forecatsing#############\n",
    "X = np.array(df.drop(['label'],1))\n",
    "X = preprocessing.scale(X)\n",
    "X = X[:-forecast_out]\n",
    "X_lately = X[-forecast_out:]\n",
    "df.dropna(inplace=True)\n",
    "y= np.array(df['label'])\n",
    "y= y[forecast_out:]\n",
    "print(len(X),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94572260801\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size=0.2)\n",
    "clf = LinearRegression(n_jobs=-1)#no of treads that linear regresion supports for traing\n",
    "clf.fit(X_train,y_train)\n",
    "accuracy=clf.score(X_test,y_test)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 828.62436306  827.1107164   816.06314423  831.55883706  831.70057668\n  837.17370418  848.75241069  851.38986825  850.36730932  854.27546079\n  853.94288709  855.23532463  851.80227761  853.6575883   848.52968876\n  852.64695259  868.00486323  873.80498082  882.84318543  880.41998055\n  867.39987432  846.70506582  844.069087    838.86087456  841.96379663\n  844.15750101  845.99244525  853.2700277   854.13447449  854.57281877\n  859.1469348   863.3369438   864.46886843] 0.94572260801 33\n"
     ]
    }
   ],
   "source": [
    "forecast_set= clf.predict(X_lately)\n",
    "print (forecast_set, accuracy,forecast_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}